{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import nltk.sentiment.util\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "import re\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import os\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../amazon_reviews_us_Grocery_v1_00_sentiment.csv').sample(100000)\n",
    "\n",
    "df = df.drop([\"marketplace\", \"customer_id\", \"product_parent\", \"product_category\", \"review_date\"], axis=1)\n",
    "\n",
    "df = df[~df['review_body'].isnull()]\n",
    "\n",
    "df['review_body'] = df['review_headline'] + '. ' + df['review_body']\n",
    "\n",
    "df[\"health_hazard\"] = np.nan\n",
    "\n",
    "df = df.set_index(\"review_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_title</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>health_hazard</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>R2VHOYB17O07LK</th>\n",
       "      <td>B000C06G5Q</td>\n",
       "      <td>Helmut Sachers Viennese Blend Whole Bean Coffe...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Stale</td>\n",
       "      <td>Stale. I was disappointed. The coffee was stal...</td>\n",
       "      <td>-0.330556</td>\n",
       "      <td>0.461111</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R3JQQEGT80DR8H</th>\n",
       "      <td>B004Z4PVJM</td>\n",
       "      <td>KP Hula Hoops Assorted 7 Pack 150g</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>love it</td>\n",
       "      <td>love it. Try it you'll love it, from UK ships ...</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.567857</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2872ATLNGNAUL</th>\n",
       "      <td>B000RJARJS</td>\n",
       "      <td>Caykur Black Tea, Altinbas, 17.6 Ounce</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Five Stars. Very strong. Don't OD on caffeine!</td>\n",
       "      <td>0.281667</td>\n",
       "      <td>0.476667</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RU3PAH42ZD2UK</th>\n",
       "      <td>B0009F3POO</td>\n",
       "      <td>Traditional Medicinals Seasonal Herb Tea Sampl...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>More like medicine than tea.....</td>\n",
       "      <td>More like medicine than tea...... Listen, this...</td>\n",
       "      <td>0.277083</td>\n",
       "      <td>0.443750</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R25MLSQK32Y2AO</th>\n",
       "      <td>B004NB7AAK</td>\n",
       "      <td>Dog Head Lollipop Suckers (1 dz)</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>cute but the taste . . .</td>\n",
       "      <td>cute but the taste . . .. My son was having a ...</td>\n",
       "      <td>-0.028125</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R1F6FKV1YAQYMR</th>\n",
       "      <td>B004RZZM34</td>\n",
       "      <td>Kid's Kitchen Microwave Cup Cheezy Mac and Che...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Five Stars. Best Mac and Cheese ever.</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2TO1G1NXUUE0J</th>\n",
       "      <td>B004PX9JPA</td>\n",
       "      <td>Slim Jim Dare Smoked Meat Sticks, Habanero, .9...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Feelin' HOT HOT HOT - ohhh yeeahhh</td>\n",
       "      <td>Feelin' HOT HOT HOT - ohhh yeeahhh. Alright - ...</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>0.273611</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R16FZSP6TNICFM</th>\n",
       "      <td>B002YRBALU</td>\n",
       "      <td>JK Gourmet Almond Flour</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>The best almond flour</td>\n",
       "      <td>The best almond flour. This is the second time...</td>\n",
       "      <td>0.147917</td>\n",
       "      <td>0.202083</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R6P7IZ5LU8RQQ</th>\n",
       "      <td>B00EIAADO6</td>\n",
       "      <td>Jeremiah's Pick Coffee Organic Water Processed...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Best Decaf We Ever Had</td>\n",
       "      <td>Best Decaf We Ever Had. My husband is a coffee...</td>\n",
       "      <td>0.228125</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R1F9IG50QL5FKX</th>\n",
       "      <td>B001ABOB08</td>\n",
       "      <td>MOCAFE Precious Divinity Decaf Spiced Chai Tea...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Sugar and non-dairy creamer</td>\n",
       "      <td>Sugar and non-dairy creamer. First ingredient,...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.136667</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                product_id                                      product_title  \\\n",
       "review_id                                                                       \n",
       "R2VHOYB17O07LK  B000C06G5Q  Helmut Sachers Viennese Blend Whole Bean Coffe...   \n",
       "R3JQQEGT80DR8H  B004Z4PVJM                 KP Hula Hoops Assorted 7 Pack 150g   \n",
       "R2872ATLNGNAUL  B000RJARJS             Caykur Black Tea, Altinbas, 17.6 Ounce   \n",
       "RU3PAH42ZD2UK   B0009F3POO  Traditional Medicinals Seasonal Herb Tea Sampl...   \n",
       "R25MLSQK32Y2AO  B004NB7AAK                   Dog Head Lollipop Suckers (1 dz)   \n",
       "...                    ...                                                ...   \n",
       "R1F6FKV1YAQYMR  B004RZZM34  Kid's Kitchen Microwave Cup Cheezy Mac and Che...   \n",
       "R2TO1G1NXUUE0J  B004PX9JPA  Slim Jim Dare Smoked Meat Sticks, Habanero, .9...   \n",
       "R16FZSP6TNICFM  B002YRBALU                            JK Gourmet Almond Flour   \n",
       "R6P7IZ5LU8RQQ   B00EIAADO6  Jeremiah's Pick Coffee Organic Water Processed...   \n",
       "R1F9IG50QL5FKX  B001ABOB08  MOCAFE Precious Divinity Decaf Spiced Chai Tea...   \n",
       "\n",
       "                star_rating  helpful_votes  total_votes vine  \\\n",
       "review_id                                                      \n",
       "R2VHOYB17O07LK            2              0            0    N   \n",
       "R3JQQEGT80DR8H            5              1            1    N   \n",
       "R2872ATLNGNAUL            5              0            0    N   \n",
       "RU3PAH42ZD2UK             1              3           13    N   \n",
       "R25MLSQK32Y2AO            1              0            0    N   \n",
       "...                     ...            ...          ...  ...   \n",
       "R1F6FKV1YAQYMR            5              0            0    N   \n",
       "R2TO1G1NXUUE0J            4              0            0    N   \n",
       "R16FZSP6TNICFM            5              1            1    N   \n",
       "R6P7IZ5LU8RQQ             5              0            0    N   \n",
       "R1F9IG50QL5FKX            2              0            0    N   \n",
       "\n",
       "               verified_purchase                     review_headline  \\\n",
       "review_id                                                              \n",
       "R2VHOYB17O07LK                 Y                               Stale   \n",
       "R3JQQEGT80DR8H                 Y                             love it   \n",
       "R2872ATLNGNAUL                 Y                          Five Stars   \n",
       "RU3PAH42ZD2UK                  Y    More like medicine than tea.....   \n",
       "R25MLSQK32Y2AO                 Y            cute but the taste . . .   \n",
       "...                          ...                                 ...   \n",
       "R1F6FKV1YAQYMR                 Y                          Five Stars   \n",
       "R2TO1G1NXUUE0J                 Y  Feelin' HOT HOT HOT - ohhh yeeahhh   \n",
       "R16FZSP6TNICFM                 Y               The best almond flour   \n",
       "R6P7IZ5LU8RQQ                  Y              Best Decaf We Ever Had   \n",
       "R1F9IG50QL5FKX                 Y         Sugar and non-dairy creamer   \n",
       "\n",
       "                                                      review_body  polarity  \\\n",
       "review_id                                                                     \n",
       "R2VHOYB17O07LK  Stale. I was disappointed. The coffee was stal... -0.330556   \n",
       "R3JQQEGT80DR8H  love it. Try it you'll love it, from UK ships ...  0.392857   \n",
       "R2872ATLNGNAUL     Five Stars. Very strong. Don't OD on caffeine!  0.281667   \n",
       "RU3PAH42ZD2UK   More like medicine than tea...... Listen, this...  0.277083   \n",
       "R25MLSQK32Y2AO  cute but the taste . . .. My son was having a ... -0.028125   \n",
       "...                                                           ...       ...   \n",
       "R1F6FKV1YAQYMR              Five Stars. Best Mac and Cheese ever.  1.000000   \n",
       "R2TO1G1NXUUE0J  Feelin' HOT HOT HOT - ohhh yeeahhh. Alright - ...  0.216667   \n",
       "R16FZSP6TNICFM  The best almond flour. This is the second time...  0.147917   \n",
       "R6P7IZ5LU8RQQ   Best Decaf We Ever Had. My husband is a coffee...  0.228125   \n",
       "R1F9IG50QL5FKX  Sugar and non-dairy creamer. First ingredient,...  0.100000   \n",
       "\n",
       "                subjectivity  health_hazard  \n",
       "review_id                                    \n",
       "R2VHOYB17O07LK      0.461111            NaN  \n",
       "R3JQQEGT80DR8H      0.567857            NaN  \n",
       "R2872ATLNGNAUL      0.476667            NaN  \n",
       "RU3PAH42ZD2UK       0.443750            NaN  \n",
       "R25MLSQK32Y2AO      0.325000            NaN  \n",
       "...                      ...            ...  \n",
       "R1F6FKV1YAQYMR      0.300000            NaN  \n",
       "R2TO1G1NXUUE0J      0.273611            NaN  \n",
       "R16FZSP6TNICFM      0.202083            NaN  \n",
       "R6P7IZ5LU8RQQ       0.406250            NaN  \n",
       "R1F9IG50QL5FKX      0.136667            NaN  \n",
       "\n",
       "[100000 rows x 12 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "neg_reviews = df.query(\"star_rating < 4\", engine=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['br'])\n",
    "translator=str.maketrans('','',string.punctuation)\n",
    "\n",
    "#neg_reviews[\"tokened_review\"] = np.nan\n",
    "all_tokens = []\n",
    "#neg_reviews.astype({\"tokened_review\": 'object'}).dtypes\n",
    "for index, row in neg_reviews.iterrows():\n",
    "    text = row[\"review_body\"].lower()\n",
    "    text = text.translate(translator)\n",
    "    text = word_tokenize(text)\n",
    "    new_text = []\n",
    "    for token in text:\n",
    "        token = stemmer.stem(token)\n",
    "        if token not in stop_words:\n",
    "            new_text.append(token)\n",
    "    all_tokens.append(new_text)\n",
    "neg_reviews[\"tokened_review\"] = all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hazardous_words=[\"rotten\", \"mouldy\", \"moldy\", \"mold\", \"mould\", \"sick\", \"dangerous\", \"diarrhea\", \"poisoning\", \"stale\", \"contaminate\",'disease', 'disgusting', 'hospital', 'poisoning', 'hazardous', 'ill', 'mold','moldy', 'mildew', 'dangerous' 'rancid', 'smelly', 'stale', 'unhealthy', 'sick','expired','expiration']\n",
    "for word in hazardous_words:\n",
    "    neg_reviews.loc[[stemmer.stem(word) in tokened_review for tokened_review in neg_reviews[\"tokened_review\"]], \"health_hazard\"] = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labeled = len(neg_reviews.query(\"health_hazard == 1\", engine=\"python\"))\n",
    "num_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_hazardous = neg_reviews.query(\"health_hazard != 1\", engine=\"python\").sample(3*num_labeled)\n",
    "for i in list(non_hazardous.index):\n",
    "    neg_reviews.loc[i, \"health_hazard\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled = neg_reviews.query(\"health_hazard == 0 or health_hazard == 1\", engine=\"python\")\n",
    "reviews = labeled.loc[:, 'tokened_review'].values\n",
    "y = labeled.loc[:, 'health_hazard'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(reviews)):\n",
    "    reviews[i] = ' '.join(reviews[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "matrix = TfidfVectorizer(max_features=200)\n",
    "X = matrix.fit_transform(reviews).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier_g = GaussianNB()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier_l = LogisticRegression()\n",
    "\n",
    "classifier_g.fit(X_train, y_train)\n",
    "classifier_l.fit(X_train, y_train)\n",
    "\n",
    "# Predict Class\n",
    "y_pred_g = classifier_g.predict(X_test)\n",
    "y_pred_l = classifier_l.predict(X_test)\n",
    "\n",
    "# Accuracy \n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_g = accuracy_score(y_test, y_pred_g)\n",
    "accuracy_l = accuracy_score(y_test, y_pred_l)\n",
    "print(accuracy_g)\n",
    "print(accuracy_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = neg_reviews.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = neg_reviews\n",
    "trial_x = trial.loc[:,\"tokened_review\"].values\n",
    "for i in range(len(trial_x)):\n",
    "    trial_x[i] = ' '.join(trial_x[i])\n",
    "trial_X = matrix.fit_transform(trial_x).toarray()\n",
    "trial_y = classifier.predict(trial_X)\n",
    "trial_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_reviews[\"health_hazard\"] = trial_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "neg_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dangerous = neg_reviews.query(\"health_hazard == 1\", engine=\"python\")\n",
    "dangerous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA\n",
    "data_filtered = dangerous['review_body'].values\n",
    "pprint.pprint(data_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        sentence = sentence.translate(translator)\n",
    "        yield(nltk.word_tokenize(str(sentence)))\n",
    "\n",
    "data_words = list(sent_to_words(data_filtered))\n",
    "clean_data = []\n",
    "for i in range(len(data_words)):\n",
    "    clean_review = []\n",
    "    for j in range(len(data_words[i])):\n",
    "        if data_words[i][j] not in stop_words:\n",
    "            clean_review.append(data_words[i][j])\n",
    "    clean_data.append(clean_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(clean_data)\n",
    "# Create Corpus\n",
    "texts = clean_data\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# View\n",
    "pprint.pprint(corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# number of topics\n",
    "num_topics = 5\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics,\n",
    "                                       minimum_probability=0.1,\n",
    "                                       workers=3,\n",
    "                                       passes=2)\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint.pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from itertools import chain\n",
    "#lda_corpus = lda_model[corpus]\n",
    "\n",
    "# Find the threshold, let's set the threshold to be 1/#clusters,\n",
    "# To prove that the threshold is sane, we average the sum of all probabilities:\n",
    "#scores = list(chain(*[[score for topic_id,score in topic] for topic in [doc for doc in lda_corpus]]))\n",
    "#threshold = sum(scores)/len(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "data_vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "\n",
    "pyLDAvis.display(data_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
