{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Amazon Fine Food Review Analysis\n\n*The Amazon fine food reviews dataset consist of reviews of fine food from Amazon*\n1. Number of reviews: 568,454 reviews*\n1. Number of Users : 256,059 users\n1. Number of products : 74,258 products\n1. Time span of taking reviews: Oct 1999 - Oct 2012\n1. Number of attributes/column in Data: 10\n\n*Attributes: Information:*\n1. Id : Row Id\n1. ProductId : Unique identifier for the product (74258 unique values )\n2. UserId : Unqiue identifier for the user (256059 unique values)\n3. ProfileName: Profile name of the user (218418 unique values)\n4. HelpfulnessNumerator : Number of users who found the review helpful\n5. HelpfulnessDenominator : Number of users who indicated whether they found the review helpful or not\n6. Score : Rating between 1 and 5\n7. Time : Timestamp for the review\n8. Summary : Brief summary of the review (295744 unique values)\n9. Text : Text of the review (393579 unique values ) (the most useful information for predicting postive or negative) \n\n\n**Objecive**: *Given a review ,determine whether  a review is positive (Rating of 4 or 5) or negative(Rating of 1 or 2) ?*\n\nQ). **How to determine if a review is positive or negative ?**\n\n*Ans). We could use the Score/rating. A rating of 4 or 5 could be considered as a positive review and A  rating of 1 or 2 could be negative reviews. A rating of 3 neutral and can be ignored. This is the approximation and proxy way of determinig the polarity (positivity/negativity) of review.*","metadata":{}},{"cell_type":"markdown","source":"# *Loading the data *\n**The dataset is available in two forms** \n1. .csv file\n2.  SqLite Database\n\n**In order to load the database, We have used the SqLite Database as it easier to query tha data and visualise the data suffuciently.**\n*Here as we only want to get the global sentiment of the recommendation(Positve or Negative), we will purposefully ignore all the scores equal to 3 ,If the Score is above 3 ,then the recommendation or review will be set to \"positive\". Otherwise ,it will be set to \"negative\".*\n","metadata":{}},{"cell_type":"markdown","source":"# Importing Important Library","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score,auc\n\nimport string\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\n\nfrom tqdm import tqdm\nimport os \n#mterics \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading  Data","metadata":{}},{"cell_type":"code","source":"#Using the Sqlite table to read data \ncon = sqlite3.connect('/kaggle/input/amazon-fine-food-reviews/database.sqlite')\n#Filtering only Positve and Negative Reviews i.e\n# not taking into consideration those reviews with Score=3\n\nfiltered_data =pd.read_sql_query(\"\"\"SELECT * FROM Reviews WHERE SCORE!=3\"\"\",con)\n\n# Give reviews with score >3  a positive rating \"1\" and review with score <3 a negative rating \"0\"\ndef  partition(x):\n    if x<3:\n        return 0\n    return 1\n        \n#Changing reviews with score<3 to be negative and score > 3 to be positive\nactualScore=filtered_data['Score']\nPositive_Negative=actualScore.map(partition)\nfiltered_data['Score']=Positive_Negative\nprint(\"No. of data points in Dataset:\",filtered_data.shape)\nfiltered_data.head(3)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis\n### Data Cleaning : Deduplication { The most important part of cleaning data}\n*It was observed (as shown in table below ) that the reviews data  had manu duplicates entries.Hence it was neccessary to remove the deduplication in order to get unbaised results for the Analysis of the data .Following is the example given below*","metadata":{}},{"cell_type":"code","source":"display=pd.read_sql_query(\"\"\"SELECT * FROM Reviews WHERE Score!=3 AND UserID='AR5J8UI46CURR' ORDER BY \nProductId\"\"\",con)\ndisplay","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As it can be seen above the same user has multiple reviews of the same values for** **HelpfulnessNumerator, HelpfulnessDenominator, Score, Time, Summary and Text and on doing analysis it was found that**\n***ProductId=B000HDOPZG was Loacker Quadratini Vanilla Wafer Cookies, 8.82-Ounce Packages (Pack of 8)***\n\n***ProductId=B000HDL1RQ was Loacker Quadratini Lemon Wafer Cookies, 8.82-Ounce Packages (Pack of 8) and so on***\n\n***It was inferred after analysis that reviews with same parameters other than ProductId belonged to the same product just having different flavour or quantity. Hence in order to reduce redundancy it was decided to eliminate the rows having same parameters.***\n\n***The method used for the same was that we first sort the data according to ProductId and then just keep the first similar product review and delelte the others.\nfor eg. in the above just the review for ProductId=B000HDL1RQ remains.\nThis method ensures that there is only one representative for each product and deduplication without sorting would lead to possibility of different representatives still existing for the same product***","metadata":{}},{"cell_type":"code","source":"# Sorting data sccording to ProducID  in ascending order\nsorted_data=filtered_data.sort_values('ProductId',axis=0,ascending=True,inplace=False,kind='quicksort',na_position='last')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Deduplication of entries\nfinal_data=sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"},keep='first',inplace=False)\nfinal_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking to see how much % of data still remains\nprint('The total data remain after cleaning data ',(final_data['Id'].size*1.0)/(filtered_data['Id'].size*1.0)*100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Observation***:It was seen that in 2 rows given below the value of HelpfullnessNumerator is Greater than HelpfullnessDenominator which is not practically hence such rows are too removed from dataset","metadata":{}},{"cell_type":"code","source":"display=pd.read_sql_query(\"\"\"SELECT * FROM Reviews WHERE Score!=3 AND Id=44737 OR Id=64422\nORDER BY ProductID\"\"\",con)\ndisplay.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_data=final_data[final_data['HelpfulnessNumerator']<=final_data['HelpfulnessDenominator']]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# before starting the next phase of text preprocessing lets see the no. of entries left\nprint(final_data.shape)\n#How many positive and negative reviews are present in our dataset?\nfinal_data['Score'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Text Preprocessing***\n**Now we have finished deduplication. Now our data requires some preprocessing before we go on futher anlysis and make the prediction model**\n*Hence in the preprocessing phase we do the following steps given below*\n1. Begin by removing html tags\n1. Removing any punctution or limited set of special character:like ,or . or # etc\n1. Check the words is made up of english letters and is not alpha-numeric\n1. Check to see if the length of the words is greater than 2 (as it was research that there is no adjective in 2 letter)\n1. Convert the words to lowercase\n1. Remove stopwords\n1. Snowball stemming the word(it is observed that Snoball stemming is better that Porter stemming)\n\n***After this we will collect the words and will use to describe positive and negative reviews.***\n","metadata":{}},{"cell_type":"code","source":"# printing some random reviews\nsent_0 = final_data['Text'].values[6]\nprint(sent_0)\nprint(\"=\"*50)\n\nsent_1000 = final_data['Text'].values[1000]\nprint(sent_1000)\nprint(\"=\"*50)\n\nsent_1500 = final_data['Text'].values[1500]\nprint(sent_1500)\nprint(\"=\"*50)\n\nsent_4900 = final_data['Text'].values[4900]\nprint(sent_4900)\nprint(\"=\"*50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re    #Tutorial about Python regular expressions: https://pymotw.com/2/re/\n# remove urls from text python : *https://stackoverflow.com/a/40823105/4084039*\nsent_0=re.sub(r'https\\S+','',sent_0)\nsent_1000=re.sub(r'https\\S+','',sent_1000)\nsent_1500=re.sub(r'https\\S+','',sent_1500)\nsent_4900=re.sub(r'http\\S+', '', sent_4900)\n\nprint(sent_0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#SENTENCES Containing HTML tags\nimport re\ni=0;\nfor sentence in final_data['Text'].values:\n    if (len(re.findall('<.*?>',sentence))):\n        print(i)\n        print(sentence)\n        break;\n    i=i+1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#stop=set(stopwords.words('english')) #set of stopwords\n# we are removing the words from the stop words list: 'no', 'nor', 'not' for semantic meanig in bigrams and trigram\nstopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn',  'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn',  'weren', \\\n            'won', \"won't\", 'wouldn'])\nstemming = nltk.stem.SnowballStemmer('english') #intialsing the snowball stemmer\n\ndef cleanhtml(sentence): # function to remove html tags from words\n    clean = re.compile('<.*?>')\n    cleantext=re.sub(clean,' ',sentence)\n    return cleantext\ndef cleanpunc(sentence): #function to clean the word of any punctuation or special characters\n    cleanp=re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    clean_punc=re.sub(r'[.|,|)|(|\\|/]',r' ',cleanp)\n    return clean_punc\n\nprint('stop words are :',stopwords)\nprint(\"*********************\")\nprint('base word for tasty is :',stemming.stem('tasty'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this code takes a while to run as it needs to run on 500k sentences.\nif not os.path.isfile('finalsqlite'):\n    final_string=[]\n    all_positive_words=[] # store words from +ve review\n    all_negative_words=[] # store words from -ve review\n    for i , sentence in enumerate(tqdm(final_data['Text'].values)):\n        filter_sentences =[]\n        sent = cleanhtml(sentence) #remove HTML tags\n        for words in sent.split():\n            # we have used cleanpunc(w).split(), one more split function here because consider w=\"abc.def\", cleanpunc(w) will return \"abc def\"\n            # if we dont use .split() function then we will be considring \"abc def\" as a single word, but if you use .split() function we will get \"abc\", \"def\"\n            for cleaned_words in cleanpunc(words).split():\n                if ((cleaned_words.isalpha()) & (len(cleaned_words)>2)):\n                    if(cleaned_words.lower() not in stopwords):\n                        s=(stemming.stem(cleaned_words.lower())).encode('utf8')\n                        filter_sentences.append(s)\n                        if (final_data['Score'].values)[i]==1:\n                            all_positive_words.append(s)  #list of all words used to describe positive reviews\n                        if (final_data['Score'].values)[i]==0:\n                            all_negative_words.append(s)  #list of all words used to describe negative reviews \n                    else:\n                        continue\n                else:\n                    continue\n        str1=b\" \".join(filter_sentences) #final string of cleaned words\n        #print(\"***********************************************************************\")\n        final_string.append(str1)\n        i+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#adding a column of CleanedText which displays the data after pre-processing of the review \nfinal_data['Cleaned_text']=final_string\nfinal_data['Cleaned_text']=final_data['Cleaned_text'].str.decode('utf-8')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Store the final table into an SQLITE table for future\nconnection=sqlite3.connect('final.sqlite')\nc=connection.cursor\nconnection.text_factory=str\nfinal_data.to_sql('Reviews',connection,schema=None,if_exists='replace',index=True,index_label=True,chunksize=None,dtype=None)\nconnection.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sorting dataset based on 'Time' feature","metadata":{}},{"cell_type":"code","source":"# converting time in unit=sec\nfinal_data['Time']=pd.to_datetime(final_data['Time'],unit='s')\nfinal_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_data=final_data.sort_values(by='Time')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Apply Knn(brute force version) on these feature sets\n * Review text, preprocessed one converted into vectors using (BOW)\n * Review text, preprocessed one converted into vectors using (TFIDF)\n * Review text, preprocessed one converted into vectors using (AVG W2v)\n\n2. Apply Knn(kd tree version) on these feature sets:    \nsklearn implementation of kd-tree accepts only dense matrices, you need to convert the sparse matrices of CountVectorizer/TfidfVectorizer into dense matices. You can convert sparse matrices to dense using .toarray() attribute. \n    * Review text, preprocessed one converted into vectors using (BOW) but with restriction on maximum features generated.\n            count_vect = CountVectorizer(min_df=10, max_features=500) \n            count_vect.fit(preprocessed_reviews)\n            \n    * Review text, preprocessed one converted into vectors using (TFIDF) but with restriction on maximum features generated.\n                tf_idf_vect = TfidfVectorizer(min_df=10, max_features=500)\n                tf_idf_vect.fit(preprocessed_reviews)\n            \n    * Review text, preprocessed one converted into vectors using (AVG W2v)\n    * Review text, preprocessed one converted into vectors using (TFIDF W2v)\n\n3. The hyper paramter tuning(find best K)\n    * Find the best hyper parameter which will give the maximum AUC value\n    * Find the best hyper paramter using k-fold cross validation or simple cross validation data\n    * Use gridsearch cv or randomsearch cv or you can also write your own for loops to do this task of hyperparameter tuning\n\n4. Representation of results\n    * You need to plot the performance of model both on train data and cross validation data for each hyper parameter, like shown in the figure\n    * Once after you found the best hyper parameter, you need to train your model with it, and find the AUC on test data and plot the ROC curve on both train and test.\n    * Along with plotting ROC curve, you need to print the confusion matrix with predicted and original labels of test data points\n5. Conclusion\n    * summarize the results at the end of the notebook, summarize it in the table format.\n\n\n1. There will be an issue of data-leakage if you vectorize the entire data and then split it into train/cv/test.\n2. To avoid the issue of data-leakag, make sure to split your data first and then vectorize it.\n3. While vectorizing your data, apply the method fit_transform() on you train data, and apply the method transform() on cv/test data.","metadata":{}},{"cell_type":"code","source":"# Counting no. of 0 and 1 in dataset\nfinal_data['Score'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# taking equal no. of negative and postive data point\ndata_pos = final_data[final_data[\"Score\"] == 1].sample(n = 60000)\ndata_neg = final_data[final_data['Score'] == 0].sample(n = 57000)\nfinal=pd.concat([data_pos,data_neg])\nfinal.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=final['Score']\nX=final['Cleaned_text']\nprint(\"Shape of X\",X.shape)\nprint(\"Shape of y\",y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_auc_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# Splitting the data in train ,test and Cv dataset\nx, X_test, y, y_test = train_test_split(X, y, test_size=.30, random_state=0)\nX_train, X_cv, y_train, y_cv = train_test_split(x, y, test_size=.30, random_state=0)\nprint('Shape of X_train is :',X_train.shape)\nprint('Shape of y_train is :',y_train.shape)\nprint(\"****\"*6)\nprint('Shape of X_Cv is :',X_cv.shape)\nprint('Shape of y_cv is :',y_cv.shape)\nprint(\"****\"*6)\nprint('Shape of X_test is :',X_test.shape)\nprint('Shape of y_test is :',y_test.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Applying KNN brute force on BOW","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n# Converting text into vector Using BOW\ncoun_vect=CountVectorizer()\ncoun_vect.fit(X_train) # fit has to happen only on train data\n\n# we use the fiited Countvectorizer to convert the text into vectors\nX_train_bow=coun_vect.transform(X_train)\nX_cv_bow=coun_vect.transform(X_cv)\nX_test_bow=coun_vect.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"vectorizer = CountVectorizer()\n\nvectorizer.fit_transform(corpus)\n\npd.DataFrame(data = vectorizer.fit_transform(corpus).toarray(),\n             columns = vectorizer.get_feature_names(), \n             index = ['v1', 'v2', 'v3']).T.to_dict()\n\nvectorizer = CountVectorizer(ngram_range=(2,2))\n\nvectorizer.fit_transform(corpus)\n\npd.DataFrame(data = vectorizer.fit_transform(corpus).toarray(), \n             columns = vectorizer.get_feature_names(), \n             index = ['v1', 'v2', 'v3']).T.to_dict()*        ","metadata":{}},{"cell_type":"code","source":"print(\"After vectorizations\")\nprint(X_train_bow.shape, y_train.shape)\nprint(X_cv_bow.shape, y_cv.shape)\nprint(X_test_bow.shape, y_test.shape)\nprint(\"=\"*100)\n\n\n\nprint(\"YOU SHOULD NOT DO SOMETHING LIKE THIS\")\n#vectorizer = CountVectorizer()\n#x_train_bow = vectorizer.fit_transform(X_train)\n#x_cv_bow = vectorizer.fit_transform(X_cv)\n#x_test_bow = vectorizer.fit_transform(X_test)\n\n#print(x_train_bow.shape, y_train.shape)\n#print(x_cv_bow.shape, y_cv.shape)\n#print(x_test_bow.shape, y_test.shape)\n\nprint(\"NOTE: THE NUMBER OF COLUMNS IN EACH OF THE VECTOR WONT BE SAME\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<pre>\n<h2> <font color='red'>YOU SHOULD NOT DO LIKE THIS </font></h2>\n1.  <font color='red'>THE VOCABULARY SHOULD BUILT ONLY WITH THE WORDS OF TRAIN DATA</font>\n    vectorizer = CountVectorizer()\n    x_train_bow = vectorizer.fit_transform(X_train)\n    x_cv_bow = vectorizer.fit_transform(X_cv)\n    x_test_bow = vectorizer.fit_transform(X_test)\n    \n2.  <font color='red'>DATA LEAKAGE PROBLEM: IF WE DO LIKE THIS WE ARE LOOKING AT THE TEST DATA BEFORE MODELING</font>\n    vectorizer = CountVectorizer()\n    X_bow = vectorizer.fit_transfomr(X)\n    X_train, X_test, y_train, y_test = train_test_split(X_bow, Y, test_size=0.33)\n   \n3. <font color='red'>YOU SHOULD PASS THE PROBABILITY SCORES NOT THE PREDICTED VALUES</font>\n    y_pred =  neigh.predict(X)\n    roc_auc_score(y_ture,y_pred)\n\n\n</pre>","metadata":{}},{"cell_type":"markdown","source":"# Applying KNN\n\n## Hyper Parameter tuning \n","metadata":{}},{"cell_type":"code","source":"### Hyper Parameter tuning Method 1: Simple 'for' loop\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import roc_auc_score \nimport matplotlib.pyplot as plt\ntrain_auc=[]\ncv_auc=[]\nk=list(range(1,30,2))\nfor i in tqdm(k):\n    knn=KNeighborsClassifier(n_neighbors=i,weights='uniform',algorithm='brute')\n    knn.fit(X_train_bow,y_train)\n    \n    y_train_pred=knn.predict_proba(X_train_bow)[:,1]\n    y_cv_pred=knn.predict_proba(X_cv_bow)[:,1]\n    \n    train_auc.append(roc_auc_score(y_train,y_train_pred))\n    cv_auc.append(roc_auc_score(y_cv,y_cv_pred))\n    \nplt.plot(k,train_auc,label='Train_auc')\nplt.plot(k,cv_auc,label='Cv_auc')\nplt.legend()\nplt.xlabel(\"K: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hypermater tunning using 10-fold CV\nk=list(range(1,30,2))\n#empty list that will hold cv scores\ncv_scores=[]\n# perform 10-fold CV\nfor i in tqdm(k):\n    knn=KNeighborsClassifier(n_neighbors=i,algorithm='brute')\n    score=cross_val_score(knn,X_train_bow,y_train,cv=10,scoring='accuracy',n_jobs=-1)\n    cv_scores.append(score.mean())\n\n    \n#determining best k\nMSE=[1- x for x in cv_scores]\noptimal_k=k[MSE.index(min(MSE))]\nprint(\"_\" * 101)\nprint(\"Optimal number of neighbors: \", optimal_k)\nprint(\"_\" * 101)\nprint(\"Missclassification error for each k values: \", np.round(MSE, 3))\nprint(\"_\" * 101)\n\n# plot error vs k \nplt.plot(k,MSE)\nplt.title(\"Number of neighbors and error\")\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Missclassification error\")\nplt.grid()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ============================== KNN with k = optimal_k ===============================================\n# learning model k = optimal\nknn_optimal_model=KNeighborsClassifier(n_neighbors=optimal_k,algorithm='brute')\nknn_optimal_model.fit(X_train_bow,y_train)\ny_test_predict=knn_optimal_model.predict_proba(X_test_bow)[:,1]\nfpr1,tpr1,threshold1=metrics.roc_curve(y_test,y_test_predict)\n\ny_tr_pred=knn_optimal_model.predict_proba(X_train_bow)[:,1]\nfpr2,tpr2,threshold2=metrics.roc_curve(y_train,y_train_pred)\n\nfig=plt.figure()\nax=plt.subplot(111)\nax.plot(fpr1, tpr1, label='Test ROC ,auc='+str(roc_auc_score(y_test,y_test_predict)))\nax.plot(fpr2, tpr2, label='Train ROC ,auc='+str(roc_auc_score(y_train,y_train_pred)))\nplt.title('ROC')\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nax.legend()\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#confusion matrix \nfrom sklearn.metrics import confusion_matrix\nknn_model=KNeighborsClassifier(n_neighbors=optimal_k,weights='uniform',algorithm='brute',metric='cosine')\nknn_model.fit(X_train_bow,y_train)\ny_test_pred=knn_model.predict(X_test_bow)\n\nimport seaborn as sns\nconf_mat=confusion_matrix(y_test,y_test_pred)\nclass_label=['negative','positive']\ndf=pd.DataFrame(conf_mat,index=class_label,columns=class_label)\nsns.heatmap(df,annot=True,fmt='d')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Applying KNN brute force on TF-idf vectors","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n# Conveting text into vectors using Tfidfvectorizer\ntfidf_count_vect=TfidfVectorizer(ngram_range=(1,2))\ntfidf_count_vect.fit(X_train)\n# we use the fited Countvectorizer to convert the text into vectors\nX_train_tfidf=tfidf_count_vect.transform(X_train)\nX_cv_tfidf=tfidf_count_vect.transform(X_cv)\nX_test_tfidf=tfidf_count_vect.transform(X_test)\n\nprint(\"After vectorizations\")\nprint(X_train_tfidf.shape, y_train.shape)\nprint(X_cv_tfidf.shape, y_cv.shape)\nprint(X_test_tfidf.shape, y_test.shape)\nprint(\"=\"*100)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Hyper Parameter tuning Method 1: Simple 'for' loop\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\ntrain_auc=[]\ncv_auc=[]\nk=list(range(1,30,2))\nfor i in tqdm(k):\n    knn_tfidf=KNeighborsClassifier(n_neighbors=i,algorithm='brute',weights='uniform')\n    knn_tfidf.fit(X_train_tfidf,y_train)\n    \n    y_cv_pred=knn_tfidf.predict_proba(X_cv_tfidf)[:,1]\n    cv_auc.append(roc_auc_score(y_cv,y_cv_pred))\n    \n    y_tr_pred=knn_tfidf.predict_proba(X_train_tfidf)[:,1]\n    train_auc.append(roc_auc_score(y_train,y_tr_pred))\n    \nplt.plot(k,train_auc,label='Train_auc')\nplt.plot(k,cv_auc,label='cv_auc')\nplt.legend()\nplt.xlabel(\"K: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.show()\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hypermater tunning using 10-fold CV\nk=list(range(1,30,2))\n#empty list that will hold cv scores\ncv_scores=[]\n# perform 10-fold CV\nfor i in tqdm(k):\n    knn=KNeighborsClassifier(n_neighbors=i,algorithm='brute')\n    score=cross_val_score(knn,X_train_tfidf,y_train,cv=10,scoring='accuracy',n_jobs=-1)\n    cv_scores.append(score.mean())\n#determining best k\nMSE=[1- x for x in cv_scores]\noptimal_k=k[MSE.index(min(MSE))]\nprint(\"_\" * 101)\nprint(\"Optimal number of neighbors: \", optimal_k)\nprint(\"_\" * 101)\nprint(\"Missclassification error for each k values: \", np.round(MSE, 3))\nprint(\"_\" * 101)\n\n# plot error vs k \nplt.plot(k,MSE)\nplt.title(\"Number of neighbors and error\")\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Missclassification error\")\nplt.grid()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ============================== KNN with k = optimal_k ===============================================\n# learning model k = optimal\nknn_tfidf_model=KNeighborsClassifier(n_neighbors=optimal_k,algorithm='brute')\nknn_tfidf_model.fit(X_train_tfidf,y_train)\n\ny_test_predict=knn_tfidf_model.predict_proba(X_test_tfidf)[:,1]\nfpr1,tpr1,threshold1=metrics.roc_curve(y_test,y_test_predict)\n\ny_tr_pred=knn_tfidf_model.predict_proba(X_train_tfidf)[:,1]\nfpr2,tpr2,threshold2=metrics.roc_curve(y_train,y_train_pred)\n\nfig=plt.figure()\nax=plt.subplot(111)\nax.plot(fpr1, tpr1, label='Test ROC ,auc='+str(roc_auc_score(y_test,y_test_predict)))\nax.plot(fpr2, tpr2, label='Train ROC ,auc='+str(roc_auc_score(y_train,y_train_pred)))\nplt.title('ROC')\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nax.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#confusion matrix \nfrom sklearn.metrics import confusion_matrix\nknn_tfidf_model=KNeighborsClassifier(n_neighbors=optimal_k,weights='uniform',algorithm='brute',metric='cosine')\nknn_tfidf_model.fit(X_train_tfidf,y_train)\ny_test_pred=knn_tfidf_model.predict(X_test_tfidf)\n\nimport seaborn as sns\nconf_mat=confusion_matrix(y_test,y_test_pred)\nclass_label=['negative','positive']\ndf=pd.DataFrame(conf_mat,index=class_label,columns=class_label)\nsns.heatmap(df,annot=True,fmt='d')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Applying KNN brute force on Avg_w2v ","metadata":{}},{"cell_type":"code","source":"# average Wordvec\n# Computer average Word2vec for each review\nimport gensim\nfrom gensim.models import Word2Vec  # it create dense vector\nfrom gensim.models import KeyedVectors\n\n\n#word2vec for train\nlist_sentences_train=[]\nfor sentence in X_train:\n    list_sentences_train.append(sentence.split())\n\nw2v_model=gensim.models.Word2Vec(list_sentences_train,min_count=5,size=50, workers=4)\nw2v_words=list(w2v_model.wv.vocab)\nsen_vect_train=[]\nfor sent in tqdm(list_sentences_train):\n    sent_vec=np.zeros(50)\n    cnt_words=0\n    for word  in sent:\n        if word in w2v_words:\n            vec=w2v_model.wv[word]\n            sent_vec+=vec\n            cnt_words+=1\n    if cnt_words!=0:\n        sent_vec/=cnt_words\n    sen_vect_train.append(sent_vec)\nprint(len(sen_vect_train))\n\nprint(len(sen_vect_train[0]))\n                \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#word2vec for Cv data\nlist_sentences_cv=[]\nfor sentence in X_cv:\n    list_sentences_cv.append(sentence.split())\n\nw2v_model=gensim.models.Word2Vec(list_sentences_cv,min_count=5,size=50, workers=4)\nw2v_words=list(w2v_model.wv.vocab)\nsen_vect_cv=[]\nfor sent in tqdm(list_sentences_cv):\n    sent_vec=np.zeros(50)\n    cnt_words=0\n    for word  in sent:\n        if word in w2v_words:\n            vec=w2v_model.wv[word]\n            sent_vec+=vec\n            cnt_words+=1\n    if cnt_words!=0:\n        sent_vec/=cnt_words\n    sen_vect_cv.append(sent_vec)\nprint(len(sen_vect_cv))\nprint(len(sen_vect_cv[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#word2vec for test data\nlist_sentences_test=[]\nfor sentence in X_test:\n    list_sentences_test.append(sentence.split())\n\nw2v_model=gensim.models.Word2Vec(list_sentences_test,min_count=5,size=50, workers=4)\nw2v_words=list(w2v_model.wv.vocab)\nsen_vect_test=[]\nfor sent in tqdm(list_sentences_test):\n    sent_vec=np.zeros(50)\n    cnt_words=0\n    for word  in sent:\n        if word in w2v_words:\n            vec=w2v_model.wv[word]\n            sent_vec+=vec\n            cnt_words+=1\n    if cnt_words!=0:\n        sent_vec/=cnt_words\n    sen_vect_test.append(sent_vec)\nprint(len(sen_vect_test))\nprint(len(sen_vect_test[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply knn on avg_w2vec\nX_tr=sen_vect_train\nX_cv=sen_vect_cv\nX_test=sen_vect_test\nauc_cv=[]\nauc_train=[]\nk=list(range(1,30,2))\nfor i in tqdm(k):\n    knn_avg_w2vec=KNeighborsClassifier(n_neighbors=i,weights='uniform',algorithm='brute')\n    knn_avg_w2vec.fit(X_tr,y_train)\n    \n    y_cv_pred=knn_avg_w2vec.predict_proba(X_cv)[:,1]\n    auc_cv.append(roc_auc_score(y_cv,y_cv_pred))\n    \n    y_tr_pred=knn_avg_w2vec.predict_proba(X_tr)[:,1]\n    auc_train.append(roc_auc_score(y_train,y_tr_pred))\nfig=plt.figure()\nax=plt.subplot(111)\nax.plot(k, auc_train, label='AUC train')\nax.plot(k, auc_cv, label='AUC CV')\nplt.title('AUC vs K')\nplt.xlabel('K')\nplt.ylabel('AUC')\nax.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hypermater tunning using 10-fold CV\nk=list(range(1,30,2))\n#empty list that will hold cv scores\ncv_scores=[]\n# perform 10-fold CV\nfor i in tqdm(k):\n    knn_avg_w2vec=KNeighborsClassifier(n_neighbors=i,weights='uniform',algorithm='brute')\n    score=cross_val_score(knn_avg_w2vec,X_tr,y_train,cv=10,scoring='accuracy',n_jobs=-1)\n    cv_scores.append(score.mean())\n\n    \n#determining best k\nMSE=[1- x for x in cv_scores]\noptimal_k=k[MSE.index(min(MSE))]\nprint(\"_\" * 101)\nprint(\"Optimal number of neighbors: \", optimal_k)\nprint(\"_\" * 101)\nprint(\"Missclassification error for each k values: \", np.round(MSE, 3))\nprint(\"_\" * 101)\n\n# plot error vs k \nplt.plot(k,MSE)\nplt.title(\"Number of neighbors and error\")\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Missclassification error\")\nplt.grid()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ============================== KNN with k = optimal_k ===============================================\n# learning model k = optimal\nknn_optimal_model=KNeighborsClassifier(n_neighbors=optimal_k,algorithm='brute')\nknn_optimal_model.fit(X_tr,y_train)\ny_test_predict=knn_optimal_model.predict_proba(X_test)[:,1]\nfpr1,tpr1,threshold1=metrics.roc_curve(y_test,y_test_predict)\n\ny_tr_pred=knn_optimal_model.predict_proba(X_tr)[:,1]\nfpr2,tpr2,threshold2=metrics.roc_curve(y_train,y_tr_pred)\n\nfig=plt.figure()\nax=plt.subplot(111)\nax.plot(fpr1, tpr1, label='Test ROC ,auc='+str(roc_auc_score(y_test,y_test_predict)))\nax.plot(fpr2, tpr2, label='Train ROC ,auc='+str(roc_auc_score(y_train,y_tr_pred)))\nplt.title('ROC')\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nax.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#confusion matrix \nfrom sklearn.metrics import confusion_matrix\nknn_optimal_model=KNeighborsClassifier(n_neighbors=optimal_k,weights='uniform',algorithm='brute',metric='cosine')\nknn_optimal_model.fit(X_tr,y_train)\ny_test_pred=knn_optimal_model.predict(X_test)\n\nimport seaborn as sns\nconf_mat=confusion_matrix(y_test,y_test_pred)\nclass_label=['negative','positive']\ndf=pd.DataFrame(conf_mat,index=class_label,columns=class_label)\nsns.heatmap(df,annot=True,fmt='d')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Applying KNN brute force on tfidf_w2v","metadata":{}},{"cell_type":"code","source":"y=final['Score']\nX=final['Cleaned_text']\nprint(\"Shape of X\",X.shape)\nprint(\"Shape of y\",y.shape)\nfrom sklearn.model_selection import train_test_split\n# Splitting the data in train ,test and Cv dataset\nx, X_test, y, y_test = train_test_split(X, y, test_size=.30, random_state=0)\nX_train, X_cv, y_train, y_cv = train_test_split(x, y, test_size=.30, random_state=0)\nprint('Shape of X_train is :',X_train.shape)\nprint('Shape of y_train is :',y_train.shape)\nprint(\"****\"*6)\nprint('Shape of X_Cv is :',X_cv.shape)\nprint('Shape of y_cv is :',y_cv.shape)\nprint(\"****\"*6)\nprint('Shape of X_test is :',X_test.shape)\nprint('Shape of y_test is :',y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gensim\nlist_of_sentences_tr=[]\nfor sentences in tqdm(X_train):\n    list_of_sentences_tr.append(sentences.split())\nw2v_model=gensim.models.Word2Vec(list_of_sentences_tr,min_count=5,size=50, workers=4)\nw2v_words=list(w2v_model.wv.vocab)\n\ntf_idf_vect=TfidfVectorizer(ngram_range=(1,2),min_df=10,max_features=500)\ntf_idf_matrix=tf_idf_vect.fit_transform(X_train)\n\ntfidf_feat=tf_idf_vect.get_feature_names()\ndictionary=dict(zip(tf_idf_vect.get_feature_names(),list(tf_idf_vect.idf_)))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#For train Data\ntf_idf_vect_tr= []\nrow=0\nfor  sentence in tqdm(list_of_sentences_tr):\n    sentence_vect=np.zeros(50)\n    weight_sum=0\n    for word in sentence:\n        if word in w2v_words and word in tfidf_feat :\n            vec=w2v_model.wv[word]\n            tf_idf=dictionary[word]*(sentence.count(word)/len(sentence))\n            sentence_vect+=(vec * tf_idf)\n            weight_sum+=tf_idf\n    if weight_sum!=0:\n        sentence_vect /=weight_sum\n    tf_idf_vect_tr.append(sentence_vect)\n    row+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#For test Data\nlist_of_sentences_test=[]\nfor sentence in X_test:\n    list_of_sentences_test.append(sentence.split())\n\ntf_idf_vect_test= []\nrow=0\nfor sentence in tqdm(list_of_sentences_test):\n    sentence_vect=np.zeros(50)\n    weight_sum=0\n    for word in sentence:\n        if word in w2v_words and word in tfidf_feat :\n            vec=w2v_model.wv[word]\n            tf_idf=dictionary[word]*(sentence.count(word)/len(sentence))\n            sentence_vect+=(vec * tf_idf)\n            weight_sum+=tf_idf\n    if weight_sum!=0:\n        sentence_vect /=weight_sum\n    tf_idf_vect_test.append(sentence_vect)\n    row+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#For cv Data\nlist_of_sentences_cv=[]\nfor sentence in X_cv:\n    list_of_sentences_cv.append(sentence.split())\n\ntf_idf_vect_cv= []\nrow=0\nfor sentence in tqdm(list_of_sentences_cv):\n    sentence_vect=np.zeros(50)\n    weight_sum=0\n    for word in sentence:\n        if word in w2v_words and word in tfidf_feat :\n            vec=w2v_model.wv[word]\n            tf_idf=dictionary[word]*(sentence.count(word)/len(sentence))\n            sentence_vect+=(vec * tf_idf)\n            weight_sum+=tf_idf\n    if weight_sum!=0:\n        sentence_vect /=weight_sum\n    tf_idf_vect_cv.append(sentence_vect)\n    row+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Applying KNN on tfidf_avg_w2v\nx_train=tf_idf_vect_tr\nx_cv=tf_idf_vect_cv\nx_test=tf_idf_vect_test\nauc_cv=[]\nauc_train=[]\nk=list(range(1,30,2))\nfor i in tqdm(k):\n    knn=KNeighborsClassifier(n_neighbors=i,weights='uniform',algorithm='brute')\n    knn.fit(x_train,y_train)\n    \n    y_cv_pred=knn.predict_proba(x_cv)[:,1]\n    auc_cv.append(roc_auc_score(y_cv,y_cv_pred))\n    \n    y_tr_pred=knn.predict_proba(x_train)[:,1]\n    auc_train.append(roc_auc_score(y_train,y_tr_pred))\nfig=plt.figure()\nax=plt.subplot(111)\nax.plot(k, auc_train, label='AUC train')\nax.plot(k, auc_cv, label='AUC CV')\nplt.title('AUC vs K')\nplt.xlabel('K')\nplt.ylabel('AUC')\nax.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# finding best k using K-fold croos validation\nk=list(range(1,50,2))\n\n#empty list to hold CV scores/accuracy\ncv_scores=[]\n\n# performing 10 fold cross validation\nfor i in tqdm(k):\n    knn = KNeighborsClassifier(n_neighbors=i,algorithm='brute',weights='uniform')\n    scores= cross_val_score(knn,x_train,y_train,cv=10,scoring='accuracy')\n    cv_scores.append(scores.mean())\n    \n#Changing to missclassification\nMSE=[1- x for x in cv_scores]\n\n#determining best K\noptimal_k=k[MSE.index(min(MSE))]\nprint('\\nThe optimal number of neighbors is %d.' % optimal_k)\n\n\n# plot misclassification error vs k \nplt.plot(k,MSE)\n\nfor xy in zip(k,np.round(MSE,3)):\n    plt.annotate('(%s, %s)' % xy, xy=xy, textcoords='data')\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()\n\nprint(\"the misclassification error for each k value is : \", np.round(MSE,3))\nprint('*********************************************************************')\nprint(\"optimal k value is: \",optimal_k)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ============================== KNN with k = optimal_k ===============================================\n# learning model k = optimal\n#import metrics\nx_test=tf_idf_vect_test\nknn_optimal_model=KNeighborsClassifier(n_neighbors=optimal_k,algorithm='brute')\nknn_optimal_model.fit(x_train,y_train)\ny_test_predict=knn_optimal_model.predict_proba(x_test)[:,1]\nfpr1,tpr1,threshold1=metrics.roc_curve(y_test,y_test_predict)\n\ny_tr_pred=knn_optimal_model.predict_proba(x_train)[:,1]\nfpr2,tpr2,threshold2=metrics.roc_curve(y_train,y_tr_pred)\n\nfig=plt.figure()\nax=plt.subplot(111)\nax.plot(fpr1, tpr1, label='Test ROC ,auc='+str(roc_auc_score(y_test,y_test_predict)))\nax.plot(fpr2, tpr2, label='Train ROC ,auc='+str(roc_auc_score(y_train,y_tr_pred)))\nplt.title('ROC')\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nax.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#confusion matrix \nfrom sklearn.metrics import confusion_matrix\nknn_=KNeighborsClassifier(n_neighbors=optimal_k,weights='uniform',algorithm='brute',metric='cosine')\nknn_.fit(x_train,y_train)\ny_test_pred=knn_.predict(x_test)\n\nimport seaborn as sns\nconf_mat=confusion_matrix(y_test,y_test_pred)\nclass_label=['negative','positive']\ndf=pd.DataFrame(conf_mat,index=class_label,columns=class_label)\nsns.heatmap(df,annot=True,fmt='d')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Applying KNN (kd_tree)","metadata":{}},{"cell_type":"code","source":"# importing important library \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import model_selection\nfrom sklearn.metrics import roc_auc_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=final['Score']\nX=final['Cleaned_text']\nprint(\"Shape of X\",X.shape)\nprint(\"Shape of y\",y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# Splitting the data in train ,test and Cv dataset\nx, X_test, y, y_test = train_test_split(X, y, test_size=.30, random_state=0)\nX_train, X_cv, y_train, y_cv = train_test_split(x, y, test_size=.30, random_state=0)\nprint('Shape of X_train is :',X_train.shape)\nprint('Shape of y_train is :',y_train.shape)\nprint(\"****\"*6)\nprint('Shape of X_Cv is :',X_cv.shape)\nprint('Shape of y_cv is :',y_cv.shape)\nprint(\"****\"*6)\nprint('Shape of X_test is :',X_test.shape)\nprint('Shape of y_test is :',y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom sklearn.feature_extraction.text import CountVectorizer\n# Converting text into vector Using BOW\ncoun_vect=CountVectorizer(min_df=10,max_features=500)\ncoun_vect.fit(X_train) # fit has to happen only on train data\n\n# we use the fiited Countvectorizer to convert the text into vectors\ntrain_bow=coun_vect.transform(X_train)\ncv_bow=coun_vect.transform(X_cv)\ntest_bow=coun_vect.transform(X_test)\n#sklearn implementation of kd-tree accepts only dense matrices,\n#you need to convert the sparse matrices of CountVectorizer/TfidfVectorizer into dense matices. \n#You can convert sparse matrices to dense using .toarray() attribute.\nX_train_bow=train_bow.toarray()\nX_cv_bow=cv_bow.toarray()\nX_test_bow=test_bow.toarray()\nprint(\"After vectorizations\")\nprint(X_train_bow.shape, y_train.shape)\nprint(X_cv_bow.shape, y_cv.shape)\nprint(X_test_bow.shape, y_test.shape)\nprint(\"=\"*100)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Hyper Parameter tuning Method 1: Simple 'for' loop\nfor i in range(1,30,2):\n    # k = 1,3,5,7,...29\n    knn = KNeighborsClassifier(n_neighbors=i,weights='uniform',algorithm='kd_tree')\n    \n    \n    # fit the modelon CV Train\n    knn.fit(X_train_bow, y_train)\n    \n    #Taking Cv data and passing it to the trained data to predict\n    pred = knn.predict(X_cv_bow)\n    \n    # Evaluating CV accuracy i.e Model Accuracy\n    # y_cv is actual label and pred is predicted label\n    acc = accuracy_score(y_cv, pred, normalize=True) * float(100)\n    print('\\n CV Accuracy for k = %d is %d %%' %(i,acc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}